import os
import re
import ast
import config
import dashscope
from dashscope import MultiModalConversation

# --- å…¨å±€å˜é‡åˆå§‹ä¸º Noneï¼Œå®ç°æ‡’åŠ è½½ ---
model = None
tokenizer = None
client = None

def init_llm():
    """åªæœ‰åœ¨ç¬¬ä¸€æ¬¡è°ƒç”¨ chat æ—¶ï¼Œæ‰ä¼šçœŸæ­£æ‰§è¡Œè¿™é‡Œçš„åˆå§‹åŒ–"""
    global model, tokenizer, client
    
    # é˜²æ­¢é‡å¤åˆå§‹åŒ–
    if client is not None or model is not None:
        return

    if getattr(config, "USE_API", False):
        print(f"ğŸŒ [LLM] æ­£åœ¨åˆå§‹åŒ– API å®¢æˆ·ç«¯: {config.MODEL_NAME}...")
        from openai import OpenAI  # ç§»åŠ¨åˆ°å‡½æ•°å†…ï¼ŒåŠ é€Ÿè„šæœ¬å¯åŠ¨
        client = OpenAI(
            api_key=config.DEEPSEEK_API_KEY, 
            base_url=config.DEEPSEEK_BASE_URL
        )
    else:
        print("ğŸš€ [LLM] æ­£åœ¨åŠ è½½æœ¬åœ°æ˜¾å¡å¼•æ“ (è¿™å¯èƒ½éœ€è¦ 30-60 ç§’)...")
        import torch # ç§»åŠ¨åˆ°å†…éƒ¨
        from transformers import AutoTokenizer, AutoModelForCausalLM
        
        tokenizer = AutoTokenizer.from_pretrained(config.LLM_PATH, trust_remote_code=True)
        model = AutoModelForCausalLM.from_pretrained(
            config.LLM_PATH,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        ).eval()

def chat(prompt: str, system_prompt: str = "ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„å­¦æœ¯åŠ©æ‰‹ã€‚", max_new_tokens=512) -> str:
    # ğŸ¯ åªæœ‰åœ¨è¿™é‡Œè¢«è°ƒç”¨æ—¶ï¼Œæ‰ä¼šå»åŠ è½½æ¨¡å‹
    init_llm()
    
    if config.USE_API:
        try:
            response = client.chat.completions.create(
                model=config.MODEL_NAME,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=max_new_tokens,
                temperature=0.1
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            return f"âŒ API å¤±è´¥: {str(e)}"
    else:
        # æœ¬åœ°æ¨¡å‹æ¨ç†é€»è¾‘
        messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": prompt}]
        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        inputs = tokenizer(text, return_tensors="pt").to(model.device)
        import torch
        with torch.no_grad():
            outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)
        return tokenizer.decode(outputs[0][inputs["input_ids"].shape[-1]:], skip_special_tokens=True).strip()

def filter_by_text(user_query: str, candidates: list) -> list:
    """çº§è”ç²¾æ’ï¼šæ–‡æœ¬é¢„åˆ¤å±‚"""
    if not candidates: return []
    print(f"ğŸ§  [Rerank] DeepSeek æ­£åœ¨è¿›è¡Œæ„å›¾å¯¹é½...")

    context_list = [f"ç¼–å· {i}: {c['description'][:100]}" for i, c in enumerate(candidates)]
    context_text = "\n".join(context_list)

    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªæå…¶èªæ˜çš„å›¾åƒæœç´¢ä¸“å®¶ã€‚ç”¨æˆ·å½“å‰çš„æœç´¢æ„å›¾æ˜¯ï¼š'{user_query}'
    ã€é‡è¦å‡†åˆ™ã€‘ï¼š
    1. è¯­ä¹‰ä¼˜å…ˆï¼šå¦‚æœç”¨æˆ·æœâ€œèŠ±â€ï¼ŒæŒ‡çš„æ˜¯è‡ªç„¶ç•Œè§‚èµæ€§çš„èŠ±æœµã€‚
    2. æ’é™¤å¹²æ‰°ï¼šä¸¥æ ¼æ’é™¤åç§°åŒ…å«è¯¥å­—ä½†å±äºå…¶ä»–ç±»åˆ«çš„ç‰©ä½“ï¼ˆå¦‚è¥¿å…°èŠ±ã€èŠ±ç”Ÿã€ç«èŠ±ç­‰ï¼‰ã€‚
    ä¸‹é¢æ˜¯ {len(candidates)} å¼ å›¾ç‰‡çš„æè¿°ï¼š
    {context_text}
    è¯·é€‰å‡ºã€çœŸæ­£ç¬¦åˆæ„å›¾ã€‘çš„ç¼–å·ã€‚è¾“å‡ºæ ¼å¼ï¼š[ç¼–å·, ç¼–å·, ...]ï¼Œä¸è¦è§£é‡Šã€‚
    """
    
    # è¿™é‡Œä¼šè‡ªåŠ¨è§¦å‘ init_llm()
    raw_response = chat(prompt, system_prompt="ä½ æ˜¯ä¸€ä¸ªå…·å¤‡å¸¸è¯†çš„è¯­ä¹‰è¿‡æ»¤å™¨ã€‚")

    try:
        match = re.search(r'\[.*\]', raw_response)
        if match:
            selected_indices = ast.literal_eval(match.group())
            return [candidates[i] for i in selected_indices if i < len(candidates)]
        return candidates[:5]
    except:
        return candidates[:5]

def describe_image(image_path: str) -> str:
    """è§†è§‰æè¿°ï¼šè°ƒç”¨ Qwen-VL APIï¼ˆè¿™ä¸ªä¸æ¶‰åŠæœ¬åœ° LLM åŠ è½½ï¼‰"""
    print(f"ğŸ‘ï¸ [Vision] æ­£åœ¨æå–å…³é”®ç‚¹: {os.path.basename(image_path)}")
    prompt = "è¯·ä¸ºå›¾ç‰‡ç”Ÿæˆè¯¦ç»†çš„ä¸­æ–‡æè¿°æ ‡ç­¾ï¼Œä»¥é€—å·åˆ†éš”ï¼Œä¸è¦è¾“å‡ºé•¿å¥ã€‚"
    messages = [{"role": "user", "content": [{"image": f"file://{image_path}"}, {"text": prompt}]}]
    
    try:
        response = MultiModalConversation.call(api_key=config.QWEN_API_KEY, model='qwen-vl-plus', messages=messages, max_tokens=50)
        return response.output.choices[0].message.content[0]['text'].strip().replace("ã€‚", "") if response.status_code == 200 else "æå–å¤±è´¥"
    except Exception as e:
        return f"å¼‚å¸¸: {str(e)}"

def verify_image_content(image_path: str, user_query: str) -> bool:
    print(f"ğŸ§ [Rerank] AI æ­£åœ¨æ·±åº¦æ ¸å¯¹: {os.path.basename(image_path)}")
    
    # ğŸ¯ æ ¸å¿ƒæ”¹åŠ¨ï¼šåŠ å…¥â€œå®½å®¹åº¦â€å¼•å¯¼ï¼Œå¹¶è¦æ±‚å®ƒæ‰¾å±€éƒ¨ç‰¹å¾
    check_prompt = f"""
    è¯·ä»”ç»†è§‚å¯Ÿå›¾ç‰‡ã€‚ç”¨æˆ·æƒ³æ‰¾çš„å†…å®¹æ˜¯ï¼š'{user_query}'ã€‚
    å¦‚æœå›¾ä¸­ã€å­˜åœ¨ã€‘ç›¸å…³å†…å®¹ï¼ˆå“ªæ€•æ˜¯èƒŒæ™¯ã€å±€éƒ¨ã€æˆ–è€…è¾ƒå°ï¼‰ï¼Œè¯·å›ç­” 'Yes'ã€‚
    å¦‚æœå®Œå…¨ä¸ç›¸å…³ï¼Œè¯·å›ç­” 'No'ã€‚
    è¯·ç›´æ¥è¾“å‡ºç»“æœï¼Œä¸è¦è§£é‡Šã€‚
    """
    
    messages = [{"role": "user", "content": [
        {"image": f"file://{os.path.abspath(image_path)}"},
        {"text": check_prompt}
    ]}]
    
    try:
        response = MultiModalConversation.call(
            api_key=config.QWEN_API_KEY, 
            model='qwen-vl-plus', 
            messages=messages, 
            max_tokens=10
        )
        res = response.output.choices[0].message.content[0]['text'].lower()
        
        is_match = "yes" in res
        status = "âœ… é€šè¿‡" if is_match else "âŒ æ‹¦æˆª"
        print(f"   â””â”€ AI åˆ¤å®š: {res.strip()} -> {status}")
        return is_match
    except Exception as e:
        print(f"   âš ï¸ è§†è§‰æ ¸éªŒå¼‚å¸¸: {e}")
        return True # å‡ºé”™é»˜è®¤æ”¾è¡Œ